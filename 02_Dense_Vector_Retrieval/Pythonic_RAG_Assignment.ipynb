{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lElF3o5PR6ys"
   },
   "source": [
    "# Your First RAG Application: Stone Ridge Investor Letter Assistant\n",
    "\n",
    "> **Note:** Please follow the best practices outlined in the [SRHG AI Usage Guidelines](https://srhg.enterprise.slack.com/docs/T0HANKTEC/F0AB86J3A1L).\n",
    "\n",
    "In this notebook, we'll walk you through each of the components that are involved in a simple RAG application by building a **Stone Ridge Investor Letter Assistant**.\n",
    "\n",
    "Imagine having an AI assistant that can answer your questions about Stone Ridge's investment philosophy, market insights, and strategic outlook based on their investor letters - that's exactly what we'll build here. We'll be using local embeddings via HuggingFace's sentence-transformers and Anthropic's Claude for the LLM.\n",
    "\n",
    "> NOTE: This was done with Python 3.12.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CtcL8P8R6yt"
   },
   "source": [
    "## Table of Contents:\n",
    "\n",
    "- Task 1: Imports and Utilities\n",
    "- Task 2: Documents (Loading the Stone Ridge Investor Letter)\n",
    "- Task 3: Embeddings and Vectors\n",
    "- Task 4: Prompts\n",
    "- Task 5: Retrieval Augmented Generation (Building the Investor Letter Assistant)\n",
    "  - üöß Activity #1: Enhance Your Investor Letter Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Dz6GYilR6yt"
   },
   "source": [
    "Let's look at a rather complicated looking visual representation of a basic RAG application.\n",
    "\n",
    "<img src=\"https://i.imgur.com/vD8b016.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjmC0KFtR6yt"
   },
   "source": [
    "## Task 1: Imports and Utilities\n",
    "\n",
    "We're just doing some imports and enabling `async` to work within the Jupyter environment here, nothing too crazy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Z1dyrG4hR6yt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import certifi\n",
    "\n",
    "# Create a combined cert bundle with Zscaler for corporate network\n",
    "zscaler_cert = \"/Users/ari.packer/repos/sidekick/zscaler.pem\"\n",
    "combined_cert = \"/tmp/combined_certs.pem\"\n",
    "\n",
    "with open(combined_cert, \"w\") as outfile:\n",
    "    with open(certifi.where(), \"r\") as certifi_file:\n",
    "        outfile.write(certifi_file.read())\n",
    "    with open(zscaler_cert, \"r\") as zscaler_file:\n",
    "        outfile.write(zscaler_file.read())\n",
    "\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = combined_cert\n",
    "os.environ['SSL_CERT_FILE'] = combined_cert\n",
    "os.environ['CURL_CA_BUNDLE'] = combined_cert\n",
    "\n",
    "from aimakerspace.text_utils import PDFFileLoader, CharacterTextSplitter\n",
    "from aimakerspace.vectordatabase import VectorDatabase\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9OrFZRnER6yt"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0jGnpQsR6yu"
   },
   "source": [
    "## Task 2: Documents\n",
    "\n",
    "We'll be concerning ourselves with this part of the flow in the following section:\n",
    "\n",
    "<img src=\"https://i.imgur.com/jTm9gjk.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SFPWvRUR6yu"
   },
   "source": [
    "### Loading Source Documents\n",
    "\n",
    "So, first things first, we need some documents to work with.\n",
    "\n",
    "While we could work directly with the `.txt` files (or whatever file-types you wanted to extend this to) we can instead do some batch processing of those documents at the beginning in order to store them in a more machine compatible format.\n",
    "\n",
    "In this case, we're going to parse our PDF file into a single document in memory.\n",
    "\n",
    "Let's look at the relevant bits of the `PDFFileLoader` class:\n",
    "\n",
    "```python\n",
    "def load_file(self):\n",
    "        doc = pymupdf.open(self.path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        self.documents.append(text)\n",
    "```\n",
    "\n",
    "We're loading the PDF document using PyMuPDF, extracting text from each page, and storing that output in our `self.documents` list.\n",
    "\n",
    "> NOTE: We're using the Stone Ridge 2025 Investor Letter as our sample data. This content covers investment philosophy, market analysis, and strategic insights - perfect for building an investor letter assistant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ia2sUEuGR6yu",
    "outputId": "84937ecc-c35f-4c4a-a4ab-9da72625954c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_loader = PDFFileLoader(\"data\")\n",
    "documents = pdf_loader.load_documents()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bV-tj5WFR6yu",
    "outputId": "674eb315-1ff3-4597-bcf5-38ece0a812ac"
   },
   "outputs": [],
   "source": [
    "# Preprocess the document to remove unwanted sections\n",
    "# Risk disclosures were causing irrelevant context retrievals\n",
    "documents[0] = documents[0].split('Risk Disclosures')[0]\n",
    "documents[1] = documents[1].split('Risk Disclosures')[0]\n",
    "documents[2] = documents[2].split('Risk Disclosures')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHlTvCzYR6yu"
   },
   "source": [
    "### Splitting Text Into Chunks\n",
    "\n",
    "As we can see, there is one massive document.\n",
    "\n",
    "We'll want to chunk the document into smaller parts so it's easier to pass the most relevant snippets to the LLM.\n",
    "\n",
    "There is no fixed way to split/chunk documents - and you'll need to rely on some intuition as well as knowing your data *very* well in order to build the most robust system.\n",
    "\n",
    "For this toy example, we'll just split blindly on length.\n",
    "\n",
    ">There's an opportunity to clear up some terminology here, for this course we will stick to the following:\n",
    ">\n",
    ">- \"source documents\" : The `.txt`, `.pdf`, `.html`, ..., files that make up the files and information we start with in its raw format\n",
    ">- \"document(s)\" : single (or more) text object(s)\n",
    ">- \"corpus\" : the combination of all of our documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2G6Voc0jR6yv"
   },
   "source": [
    "As you can imagine (though it's not specifically true in this toy example) the idea of splitting documents is to break them into manageable sized chunks that retain the most relevant local context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMC4tsEmR6yv",
    "outputId": "08689c0b-57cd-4040-942a-8193e997f5cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(1000, 0, 'paragraph')\n",
    "split_documents = text_splitter.split_texts(documents)\n",
    "print(len(split_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2wKT0WLR6yv"
   },
   "source": [
    "Let's take a look at some of the documents we've managed to split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vcYMwWJoR6yv",
    "outputId": "20d69876-feca-4826-b4be-32915276987a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stone Ridge is most proud of the 50/50 partnership we have with you, our investors. We are on the path together. \\n\\nYou contribute the capital necessary to propel and sustain groundbreaking product development. We contribute \\n\\nour collective careers‚Äô worth of experience in sourcing, structuring, execution, and risk management. Together, it \\n\\nworks. In that spirit, I offer my deepest gratitude to you for sharing responsibility for your wealth with us this year. \\n\\nWe look forward to serving you again in 2024.\\n\\nWarmly,\\n\\nRoss L. Stevens\\n\\nFounder, CEO\\n\\nEndnotes\\n\\ni\\t 2023 returns as of 12/29/2023.\\n\\nii\\t Source: Credit Suisse, Deloitte, Masterworks, McKinsey.\\n\\niii\\t Source: Art Basel UBS 2022. Volume is auction volume plus dealer volume. Auction volumes are actuals. Dealer volumes are \\n\\nestimated by scaling auction volume by the market-wide proportion of auction versus dealer volume.',\n",
       " 'iv\\t Source for art returns: Index of public auction sales of PWC art constructed using the Case-Shiller methodology. Returns exclude any \\n\\nfund fees or expenses but include the auction buyer‚Äôs premium if applicable. Data are for the 25-year period ending 2022. Source for \\n\\nS&P 500 Returns for the same period: Bloomberg.\\n\\nv\\t Source for Damien Hirst: Artprice artist‚Äôs index. Source for the market: Index of public auction sales of PWC art constructed \\n\\nusing the Case-Shiller methodology (see above).\\n\\nvi\\t Fama, Eugene F. ‚ÄúEfficient Capital Markets: A Review of Theory and Empirical Work.‚Äù The Journal of Finance, vol. 25, no. 2, \\n\\n1970, pp. 383‚Äì417. JSTOR, https://doi.org/10.2307/2325486. Accessed 27 Dec. 2023.\\n\\nvii\\tMoney supply (M2) increased money supply (M2) from 15.3T in December 2019 to 21.7T in July 2022; currently 20.8T. \\n\\nSource: Federal Reserve Economic Data as of 11/30/2023.\\n\\nviii\\tSource: Bloomberg as of 12/26/2023.\\n\\nix\\t Source: Bloomberg as of 12/26/2023.',\n",
       " 'x\\t Source: Bloomberg as of 12/26/2023.\\n\\nxi\\t Hughes, Eric, ‚ÄúA Cypherpunk‚Äôs Manifesto,‚Äù 9 March 1993. Available at https://www.activism.net/cypherpunk/manifesto.html. \\n\\nAccessed 26 December 2023.\\n\\n___\\n\\nThe bitcoin section is inspired by the work of many authors, especially Robert Breedlove and Murray Rothbard, with certain \\n\\nconcepts and select phrases from their various writings and on-topic podcasts. The section on Stone Ridge people and \\n\\nrecruiting is, in part, inspired by, and uses select phrases from, The Chicago Canon on Free Inquiry and Expression.\\n\\n___\\n\\nStandardized returns as of most recent quarter-end (9/30/2023).\\n\\nStrategy\\n\\nCat Bond Index-Like\\n\\nFlagship\\n\\n1 Year\\n\\n24.74%\\n\\n42.28%\\n\\n5 Years\\n\\n4.64%\\n\\n4.03%\\n\\n10 Years\\n\\n4.72%\\n\\nSince Inception\\n\\n4.84%\\n\\n3.59%\\n\\nCat Bond Index-Like strategy inception 2/1/2013; Flagship strategy inception 12/9/2013. Results reflect the reinvestment of',\n",
       " 'dividends and other earnings and are net of fees and expenses. Performance data quoted represents past performance; \\n\\npast performance does not guarantee future results.',\n",
       " '2024 Investor Letter\\n\\nINVESTOR LETTER\\n\\n‚ÄúDon‚Äôt pay attention to the critics ‚Äì don‚Äôt even ignore them.‚Äù\\n\\n‚Äî  Samuel Goldwyn, penniless immigrant, producer of Hollywood‚Äôs first major motion picture \\n\\n‚ÄúYou cannot overtake 15 cars when it‚Äôs sunny, but you can when it‚Äôs raining.‚Äù\\n\\n‚Äî  Ayrton Senna, greatest Formula One driver of all time \\n\\n‚ÄúNever go to sea with two chronometers; take one or three.‚Äù\\n\\n‚Äî  Satoshi Nakamoto, comment in version 0.1 of the bitcoin code\\n\\n‚ÄúIf I skip practice one day, I notice.\\n\\nIf I skip practice two days, my wife notices.\\n\\nIf I skip practice three days, the world notices.‚Äù\\n\\n‚Äî  Vladimir Horowitz, greatest pianist of all time\\n\\n‚ÄúA wall has always been the best place to publish your work.‚Äù\\n\\n‚Äî  Banksy\\n\\nDecember 2024\\n\\nDear Fellow Investor,\\n\\nCan a page have three sides?  What if it has to?\\n\\nPassing through the initial gates of the University of Chicago‚Äôs Ph.D. program in the 1990s required passing two',\n",
       " 'brutally difficult exams, one after each of the first two years in the program.  Called preliminary exams, or ‚Äúprelims‚Äù \\n\\nfor short, they were anything but, thoroughly testing knowledge of an entire field such as management, accounting, \\n\\nfinance, or statistics.  Fail either test and get kicked out of the program.\\n\\nEntering the program straight from undergrad, amidst fellow students who already had Ph.D.s in engineering and \\n\\nphysics, I found myself immensely underpowered in statistics.  That was not ok.  I decided to take nine statistics \\n\\nclasses my first year, and none in any other field.  Total immersion.  \\n\\nWith regular classes ending late May and the ‚Äústat prelim‚Äù not until late August, I had to determine the optimal \\n\\ntime to begin studying.  Starting immediately was tempting.  12 weeks to prepare, enough time to go deep on \\n\\neverything.  However, zero chance of retaining knowledge from material I reviewed those initial weeks of study on']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_documents[50:56]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOU-RFP_R6yv"
   },
   "source": [
    "## Task 3: Embeddings and Vectors\n",
    "\n",
    "Next, we have to convert our corpus into a \"machine readable\" format as we explored in the Embedding Primer notebook.\n",
    "\n",
    "Today, we're going to talk about the actual process of creating, and then storing, these embeddings, and how we can leverage that to intelligently add context to our queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model Setup\n",
    "\n",
    "Our embeddings are generated locally using the `all-MiniLM-L6-v2` model from HuggingFace's sentence-transformers library. No API key is required for embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings run locally - no API key needed for this step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Database\n",
    "\n",
    "Let's set up our vector database to hold all our documents and their embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDQrfAR1R6yv"
   },
   "source": [
    "While this is all baked into 1 call - we can look at some of the code that powers this process to get a better understanding:\n",
    "\n",
    "Let's look at our `VectorDatabase().__init__()`:\n",
    "\n",
    "```python\n",
    "def __init__(self, embedding_model: EmbeddingModel = None):\n",
    "        self.vectors = defaultdict(np.array)\n",
    "        self.embedding_model = embedding_model or EmbeddingModel()\n",
    "```\n",
    "\n",
    "As you can see - our vectors are merely stored as a dictionary of `np.array` objects.\n",
    "\n",
    "Secondly, our `VectorDatabase()` has a default `EmbeddingModel()` which uses the `all-MiniLM-L6-v2` model from HuggingFace's sentence-transformers library, running locally on your machine.\n",
    "\n",
    "> **Quick Info About `all-MiniLM-L6-v2`**:\n",
    "> - It is a lightweight, fast embedding model\n",
    "> - It returns vectors with dimension **384**\n",
    "> - No API calls required - runs entirely locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L273pRdeR6yv"
   },
   "source": [
    "#### ‚ùìQuestion #1:\n",
    "\n",
    "The default embedding dimension of `all-MiniLM-L6-v2` is 384, as noted above. \n",
    "\n",
    "1. What are some trade-offs between smaller embedding dimensions (like 384) vs larger ones (like 1536)?\n",
    "2. How might you choose an embedding model for a specific use case?\n",
    "\n",
    "> NOTE: Consider factors like speed, memory usage, and semantic accuracy when thinking about these questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### ‚úÖ Answer:\n1. Smaller embedding dimensions use less resources (e.g., memory, compute) for the process. Smaller models behind APIs like OpenAI's are probably less expensive to use and faster. Larger models are more expensive (resource-use local, cost remote), but return more accurate results.\n2. I would go with the smallest model that is semantically accurate enough for the use case to save resources. Determining what is semantically accurate enough is difficult, and would probably require controlled testing (e.g., write a program that does a blind test by testing the same N queries against multiple models and having the final result be rated). This would allow determining if the incremental cost of the larger models provides actual value to the end result to help with a cost/benefit analysis."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5FZY7K3R6yv"
   },
   "source": [
    "We can call the `async_get_embeddings` method of our `EmbeddingModel()` on a list of `str` and receive a list of `float` back!\n",
    "\n",
    "```python\n",
    "async def async_get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return await loop.run_in_executor(None, self.get_embeddings, list_of_text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSct6X0aR6yv"
   },
   "source": [
    "We cast those to `np.array` when we build our `VectorDatabase()`:\n",
    "\n",
    "```python\n",
    "async def abuild_from_list(self, list_of_text: List[str]) -> \"VectorDatabase\":\n",
    "        embeddings = await self.embedding_model.async_get_embeddings(list_of_text)\n",
    "        for text, embedding in zip(list_of_text, embeddings):\n",
    "            self.insert(text, np.array(embedding))\n",
    "        return self\n",
    "```\n",
    "\n",
    "And that's all we need to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O4KoLbVDR6yv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327a214575dc43abb62aeba931d42d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "vector_db = VectorDatabase()\n",
    "vector_db = asyncio.run(vector_db.abuild_from_list(split_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSZwaGvpR6yv"
   },
   "source": [
    "#### ‚ùìQuestion #2:\n",
    "\n",
    "What are the benefits of using an `async` approach to collecting our embeddings?\n",
    "\n",
    "> NOTE: Determining the core difference between `async` and `sync` will be useful! If you get stuck - ask ChatGPT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### ‚úÖ Answer:\nThe benefits are obvious when the embedding model is behind a remote server, because each request that leaves the process and hits the network adds significant latency on top of the time it takes the model to actually compute the result. Embedding each chunk sequentially in this manner would be very slow, and performing remote calls asynchronously and concurrently is a basic technique used by software systems to increase throughput.\n\nThe model we're using here is not on a remote server, so the main reason to perform embedding concurrently would be to allow multiple threads to do work across multiple cores on the local machine. The HuggingFace embedding model already effectively uses multithreading, so there is little real benefit in this instance from the async approach."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRBdIt-xR6yw"
   },
   "source": [
    "So, to review what we've done so far in natural language:\n",
    "\n",
    "1. We load source documents\n",
    "2. We split those source documents into smaller chunks (documents)\n",
    "3. We embed each document using the local `all-MiniLM-L6-v2` model\n",
    "4. We store each of the text representations with the vector representations as keys/values in a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-vWANZyR6yw"
   },
   "source": [
    "### Semantic Similarity\n",
    "\n",
    "The next step is to be able to query our `VectorDatabase()` with a `str` and have it return to us vectors and text that is most relevant from our corpus.\n",
    "\n",
    "We're going to use the following process to achieve this in our toy example:\n",
    "\n",
    "1. We need to embed our query with the same `EmbeddingModel()` as we used to construct our `VectorDatabase()`\n",
    "2. We loop through every vector in our `VectorDatabase()` and use a distance measure to compare how related they are\n",
    "3. We return a list of the top `k` closest vectors, with their text representations\n",
    "\n",
    "There's some very heavy optimization that can be done at each of these steps - but let's just focus on the basic pattern in this notebook.\n",
    "\n",
    "> We are using [cosine similarity](https://www.engati.com/glossary/cosine-similarity) as a distance metric in this example - but there are many many distance metrics you could use - like [these](https://flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55)\n",
    "\n",
    "> We are using a rather inefficient way of calculating relative distance between the query vector and all other vectors - there are more advanced approaches that are much more efficient, like [ANN](https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76d96uavR6yw",
    "outputId": "bbfccc31-20a2-41c7-c14d-46554a43ed2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('for our investors across all franchises, with no down months. While everyone at Stone Ridge can be proud of those \\n\\nnumbers, perspective is vital. \\n\\nFirst, nothing we‚Äôve achieved this year is because of this year. Success, and failure, are lagging indicators. Mountain \\n\\nclimbing disasters are always a series of small, seemingly inconsequential decisions that interact in unexpected \\n\\nways, compounding exponentially. Success works the same way. \\n\\nIt is fashionable these days to say investment outcomes follow a Power Law. That‚Äôs technically true and soulless. At \\n\\nStone Ridge, the compounding we‚Äôre after is in wisdom and trust in our relationships with each other. Life is much \\n\\nmore satisfying when we realize that relationships also follow a Power Law, and invest accordingly.\\n\\nSecond, we let the simplicity of our business model be enough. The humility of our questions and the plainness of our',\n",
       "  np.float64(0.6605797911864041)),\n",
       " ('particularly around known and unknown unknowns in stressed markets. We seek ‚Äújust right‚Äù for us. \\n\\nIn alternative lending, this means no AAA student loans (too cold) and no subprime or Emerging Markets (too \\n\\nhot). Prime, U.S.-only is just right. In SFR, this means no homes over $500k (too cold) or under $150k (too hot). It \\n\\nalso means no low-growth Detroit (too cold) and no boom/bust Seattle (too hot). $250-400k homes in steady San \\n\\nAntonio, for example, are just right. \\n\\nPutting it all together, as a material source of expected firm income, we invest our firm‚Äôs balance sheet in all \\n\\nStone Ridge strategies ‚Äì alternative lending, reinsurance, SFR, energy, art, market insurance, and bitcoin ‚Äì each \\n\\ndeveloped based on the foundational principles outlined above.\\n\\nOur empirical research shows that a hypothetical portfolio of all Stone Ridge strategies has produced meaningful',\n",
       "  np.float64(0.6280629784733714)),\n",
       " ('the seed investor and, whenever possible ‚Äì but not yet always ‚Äì the largest.  Consider our ‚Äúno-device‚Äù policy in \\n\\nthis context.  Would you ever pull out your phone and start scrolling in the middle of a meeting with your largest \\n\\ninvestor?  At Stone Ridge, we do not pull our phones out on each other.\\n\\nWith Stone Ridge increasingly Stone Ridge‚Äôs largest investor, the scale of our profits as principals, not agents, creates \\n\\na level of alignment with our outside investors we are unaware exists elsewhere.  Those investors occasionally \\n\\nremark that this is a wonderfully clarifying and palpable distinction, and that Stone Ridge feels different inside. \\n\\nEarlier this year, Stone Ridge Holdings Group (SRHG), parent company of Stone Ridge Asset Management, NYDIG, \\n\\nand Longtail Re, completed a significant primary equity financing led by an iconic American financial services firm',\n",
       "  np.float64(0.6033541703786546))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db.search_by_text(\"What is Stone Ridge's investment philosophy?\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TehsfIiKR6yw"
   },
   "source": [
    "## Task 4: Prompts\n",
    "\n",
    "In the following section, we'll be looking at the role of prompts - and how they help us to guide our application in the right direction.\n",
    "\n",
    "In this notebook, we're going to rely on the idea of \"zero-shot in-context learning\".\n",
    "\n",
    "This is a lot of words to say: \"We will ask it to perform our desired task in the prompt, and provide no examples.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXpA0UveR6yw"
   },
   "source": [
    "### XYZRolePrompt\n",
    "\n",
    "Before we do that, let's stop and think a bit about how chat models work with the Messages API format.\n",
    "\n",
    "Most LLM APIs (OpenAI, Anthropic, etc.) use a similar role-based message structure:\n",
    "\n",
    "- `{\"role\" : \"system\"}` : The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation.\n",
    "- `{\"role\" : \"user\"}` : The user messages provide requests or comments for the assistant to respond to.\n",
    "- `{\"role\" : \"assistant\"}` : Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.\n",
    "\n",
    "The main idea is this:\n",
    "\n",
    "1. You start with a system message that outlines how the LLM should respond, what kind of behaviours you can expect from it, and more\n",
    "2. Then, you can provide a few examples in the form of \"assistant\"/\"user\" pairs\n",
    "3. Then, you prompt the model with the true \"user\" message.\n",
    "\n",
    "In this example, we'll be forgoing the 2nd step for simplicity's sake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdZ2KWKSR6yw"
   },
   "source": [
    "#### Utility Functions\n",
    "\n",
    "You'll notice that we're using some utility functions from the `aimakerspace` module - let's take a peek at these and see what they're doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFbeJDDsR6yw"
   },
   "source": [
    "##### XYZRolePrompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mojJSE3R6yw"
   },
   "source": [
    "Here we have our `system`, `user`, and `assistant` role prompts.\n",
    "\n",
    "Let's take a peek at what they look like:\n",
    "\n",
    "```python\n",
    "class BasePrompt:\n",
    "    def __init__(self, prompt):\n",
    "        \"\"\"\n",
    "        Initializes the BasePrompt object with a prompt template.\n",
    "\n",
    "        :param prompt: A string that can contain placeholders within curly braces\n",
    "        \"\"\"\n",
    "        self.prompt = prompt\n",
    "        self._pattern = re.compile(r\"\\{([^}]+)\\}\")\n",
    "\n",
    "    def format_prompt(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Formats the prompt string using the keyword arguments provided.\n",
    "\n",
    "        :param kwargs: The values to substitute into the prompt string\n",
    "        :return: The formatted prompt string\n",
    "        \"\"\"\n",
    "        matches = self._pattern.findall(self.prompt)\n",
    "        return self.prompt.format(**{match: kwargs.get(match, \"\") for match in matches})\n",
    "\n",
    "    def get_input_variables(self):\n",
    "        \"\"\"\n",
    "        Gets the list of input variable names from the prompt string.\n",
    "\n",
    "        :return: List of input variable names\n",
    "        \"\"\"\n",
    "        return self._pattern.findall(self.prompt)\n",
    "```\n",
    "\n",
    "Then we have our `RolePrompt` which laser focuses us on the role pattern found in most API endpoints for LLMs.\n",
    "\n",
    "```python\n",
    "class RolePrompt(BasePrompt):\n",
    "    def __init__(self, prompt, role: str):\n",
    "        \"\"\"\n",
    "        Initializes the RolePrompt object with a prompt template and a role.\n",
    "\n",
    "        :param prompt: A string that can contain placeholders within curly braces\n",
    "        :param role: The role for the message ('system', 'user', or 'assistant')\n",
    "        \"\"\"\n",
    "        super().__init__(prompt)\n",
    "        self.role = role\n",
    "\n",
    "    def create_message(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a message dictionary with a role and a formatted message.\n",
    "\n",
    "        :param kwargs: The values to substitute into the prompt string\n",
    "        :return: Dictionary containing the role and the formatted message\n",
    "        \"\"\"\n",
    "        return {\"role\": self.role, \"content\": self.format_prompt(**kwargs)}\n",
    "```\n",
    "\n",
    "We'll look at how the `SystemRolePrompt` is constructed to get a better idea of how that extension works:\n",
    "\n",
    "```python\n",
    "class SystemRolePrompt(RolePrompt):\n",
    "    def __init__(self, prompt: str):\n",
    "        super().__init__(prompt, \"system\")\n",
    "```\n",
    "\n",
    "That pattern is repeated for our `UserRolePrompt` and our `AssistantRolePrompt` as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D361R6sMR6yw"
   },
   "source": [
    "##### Chat Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJVQ2Pm8R6yw"
   },
   "source": [
    "Next we have our chat model wrapper, which provides a consistent interface for interacting with Anthropic's Claude.\n",
    "\n",
    "Let's take a peek at how that is constructed:\n",
    "\n",
    "```python\n",
    "class ChatAnthropic:\n",
    "    def __init__(self, model_name: str = \"claude-sonnet-4-5-20250929\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = os.getenv(\"ANTHROPIC_BASE_URL\")\n",
    "        self.auth_token = os.getenv(\"ANTHROPIC_AUTH_TOKEN\")\n",
    "        if self.auth_token is None:\n",
    "            raise ValueError(\"ANTHROPIC_AUTH_TOKEN is not set\")\n",
    "\n",
    "    def run(self, messages, text_only: bool = True, **kwargs):\n",
    "        # ... handles system messages separately (Anthropic API requirement)\n",
    "        # ... creates Anthropic client and returns response\n",
    "```\n",
    "\n",
    "> NOTE: Anthropic's API handles system messages differently than OpenAI - they're passed as a separate parameter rather than in the messages list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCU7FfhIR6yw"
   },
   "source": [
    "#### ‚ùì Question #3:\n",
    "\n",
    "When calling an LLM API - are there any ways we can achieve more reproducible outputs?\n",
    "\n",
    "> NOTE: Look into parameters like `temperature` and `seed` that most LLM providers support!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### ‚úÖ Answer:\nI wasn't familiar with using temperature to control the randomness of output. To achieve more reproducible outputs, temperature can be set to 0 to select the most probable token at each step (but variations still occur for reasons I don't fully understand). In order for this to work, a seed must be provided and kept consistent across each request."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5wcjMLCR6yw"
   },
   "source": [
    "### Creating and Prompting the LLM\n",
    "\n",
    "Let's tie all these together and use it to prompt our chat model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "WIfpIot7R6yw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'claude-sonnet-4-5-20250929', 'max_tokens': 1024, 'messages': [{'role': 'user', 'content': 'What is the best way to write a loop?'}], 'system': 'You are an expert in Python, you always answer in a kind way.'}\n"
     ]
    }
   ],
   "source": [
    "from aimakerspace.ai_utils.prompts import (\n",
    "    UserRolePrompt,\n",
    "    SystemRolePrompt,\n",
    "    AssistantRolePrompt,\n",
    ")\n",
    "\n",
    "from aimakerspace.ai_utils.chatmodel import ChatAnthropic\n",
    "\n",
    "chat_llm = ChatAnthropic()\n",
    "user_prompt_template = \"{content}\"\n",
    "user_role_prompt = UserRolePrompt(user_prompt_template)\n",
    "system_prompt_template = (\n",
    "    \"You are an expert in {expertise}, you always answer in a kind way.\"\n",
    ")\n",
    "system_role_prompt = SystemRolePrompt(system_prompt_template)\n",
    "\n",
    "messages = [\n",
    "    system_role_prompt.create_message(expertise=\"Python\"),\n",
    "    user_role_prompt.create_message(\n",
    "        content=\"What is the best way to write a loop?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = chat_llm.run(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dHo7lssNR6yw",
    "outputId": "1d3823fa-bb6b-45f6-ddba-b41686388324"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Best Ways to Write Loops in Python üêç\n",
      "\n",
      "The \"best\" way depends on your specific use case! Here are the most Pythonic approaches:\n",
      "\n",
      "## **1. For Loops with Iterables (Most Common)**\n",
      "```python\n",
      "# Simple iteration\n",
      "for item in my_list:\n",
      "    print(item)\n",
      "\n",
      "# With index using enumerate()\n",
      "for index, item in enumerate(my_list):\n",
      "    print(f\"{index}: {item}\")\n",
      "```\n",
      "\n",
      "## **2. List Comprehensions (When Building Lists)**\n",
      "```python\n",
      "# Instead of:\n",
      "result = []\n",
      "for x in range(10):\n",
      "    result.append(x * 2)\n",
      "\n",
      "# Use:\n",
      "result = [x * 2 for x in range(10)]\n",
      "```\n",
      "\n",
      "## **3. While Loops (For Conditional Iteration)**\n",
      "```python\n",
      "while condition:\n",
      "    # do something\n",
      "    pass\n",
      "```\n",
      "\n",
      "## **4. Built-in Functions (Often the Best Choice!)**\n",
      "```python\n",
      "# Instead of loops, consider:\n",
      "sum(my_list)           # Sum elements\n",
      "any(my_list)           # Check if any True\n",
      "all(my_list)           # Check if all True\n",
      "map(func, my_list)     # Apply function to all\n",
      "filter(func, my_list)  # Filter elements\n",
      "```\n",
      "\n",
      "## **General Tips:**\n",
      "- ‚úÖ Use `for` loops for known iterations\n",
      "- ‚úÖ Use list comprehensions for transforming data\n",
      "- ‚úÖ Use built-in functions when available (they're optimized!)\n",
      "- ‚úÖ Avoid `range(len(list))` - use `enumerate()` instead\n",
      "- ‚ùå Avoid `while True` without a clear break condition\n",
      "\n",
      "**What are you trying to loop through?** I'd be happy to suggest the most suitable approach for your specific case! üòä\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2nxxhB2R6yy"
   },
   "source": [
    "## Task 5: Retrieval Augmented Generation\n",
    "\n",
    "Now we can create a RAG prompt - which will help our system behave in a way that makes sense!\n",
    "\n",
    "There is much you could do here, many tweaks and improvements to be made!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "D1hamzGaR6yy"
   },
   "outputs": [],
   "source": [
    "RAG_SYSTEM_TEMPLATE = \"\"\"You are a helpful investor letter assistant that answers questions about Stone Ridge's investment philosophy, market insights, and strategic outlook based strictly on provided context.\n",
    "\n",
    "Instructions:\n",
    "- Only answer questions using information from the provided context\n",
    "- If the context doesn't contain relevant information, respond with \"I don't have information about that in the investor letter\"\n",
    "- Be accurate and cite specific parts of the context when possible\n",
    "- Keep responses {response_style} and {response_length}\n",
    "- Only use the provided context. Do not use external knowledge.\n",
    "- Include a reminder that this is for informational purposes only and not investment advice when appropriate\n",
    "- Only provide answers when you are confident the context supports your response.\"\"\"\n",
    "\n",
    "RAG_USER_TEMPLATE = \"\"\"Context Information:\n",
    "{context}\n",
    "\n",
    "Number of relevant sources found: {context_count}\n",
    "{similarity_scores}\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Please provide your answer based solely on the context above.\"\"\"\n",
    "\n",
    "rag_system_prompt = SystemRolePrompt(\n",
    "    RAG_SYSTEM_TEMPLATE,\n",
    "    strict=True,\n",
    "    defaults={\n",
    "        \"response_style\": \"concise\",\n",
    "        \"response_length\": \"brief\"\n",
    "    }\n",
    ")\n",
    "\n",
    "rag_user_prompt = UserRolePrompt(\n",
    "    RAG_USER_TEMPLATE,\n",
    "    strict=True,\n",
    "    defaults={\n",
    "        \"context_count\": \"\",\n",
    "        \"similarity_scores\": \"\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalAugmentedQAPipeline:\n",
    "    def __init__(self, llm: ChatAnthropic, vector_db_retriever: VectorDatabase, \n",
    "                 response_style: str = \"detailed\", include_scores: bool = False) -> None:\n",
    "        self.llm = llm\n",
    "        self.vector_db_retriever = vector_db_retriever\n",
    "        self.response_style = response_style\n",
    "        self.include_scores = include_scores\n",
    "\n",
    "    def run_pipeline(self, user_query: str, k: int = 3, **system_kwargs) -> dict:\n",
    "        # Retrieve relevant contexts\n",
    "        context_list = self.vector_db_retriever.search_by_text(user_query, k=k)        \n",
    "\n",
    "        context_prompt = \"\"\n",
    "        similarity_scores = []\n",
    "        \n",
    "        # sort context list by score descending\n",
    "        context_list = sorted(context_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        for i, (context, score) in enumerate(context_list, 1):\n",
    "\n",
    "            context_prompt += f\"[Source {i}]: {context}\\n\\n\"\n",
    "            similarity_scores.append(f\"Source {i}: {score:.3f}\")\n",
    "        \n",
    "        # Create system message with parameters\n",
    "        system_params = {\n",
    "            \"response_style\": self.response_style,\n",
    "            \"response_length\": system_kwargs.get(\"response_length\", \"detailed\")\n",
    "        }\n",
    "        \n",
    "        formatted_system_prompt = rag_system_prompt.create_message(**system_params)\n",
    "        \n",
    "        user_params = {\n",
    "            \"user_query\": user_query,\n",
    "            \"context\": context_prompt.strip(),\n",
    "            \"context_count\": len(context_list),\n",
    "            \"similarity_scores\": f\"Relevance scores: {', '.join(similarity_scores)}\" if self.include_scores else \"\"\n",
    "        }\n",
    "        \n",
    "        formatted_user_prompt = rag_user_prompt.create_message(**user_params)\n",
    "\n",
    "        return {\n",
    "            \"response\": self.llm.run([formatted_system_prompt, formatted_user_prompt]), \n",
    "            \"context\": context_list,\n",
    "            \"context_count\": len(context_list),\n",
    "            \"similarity_scores\": similarity_scores if self.include_scores else None,\n",
    "            \"prompts_used\": {\n",
    "                \"system\": formatted_system_prompt,\n",
    "                \"user\": formatted_user_prompt\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'claude-sonnet-4-5-20250929', 'max_tokens': 1024, 'messages': [{'role': 'user', 'content': \"Context Information:\\n[Source 1]: for our investors across all franchises, with no down months. While everyone at Stone Ridge can be proud of those \\n\\nnumbers, perspective is vital. \\n\\nFirst, nothing we‚Äôve achieved this year is because of this year. Success, and failure, are lagging indicators. Mountain \\n\\nclimbing disasters are always a series of small, seemingly inconsequential decisions that interact in unexpected \\n\\nways, compounding exponentially. Success works the same way. \\n\\nIt is fashionable these days to say investment outcomes follow a Power Law. That‚Äôs technically true and soulless. At \\n\\nStone Ridge, the compounding we‚Äôre after is in wisdom and trust in our relationships with each other. Life is much \\n\\nmore satisfying when we realize that relationships also follow a Power Law, and invest accordingly.\\n\\nSecond, we let the simplicity of our business model be enough. The humility of our questions and the plainness of our\\n\\n[Source 2]: particularly around known and unknown unknowns in stressed markets. We seek ‚Äújust right‚Äù for us. \\n\\nIn alternative lending, this means no AAA student loans (too cold) and no subprime or Emerging Markets (too \\n\\nhot). Prime, U.S.-only is just right. In SFR, this means no homes over $500k (too cold) or under $150k (too hot). It \\n\\nalso means no low-growth Detroit (too cold) and no boom/bust Seattle (too hot). $250-400k homes in steady San \\n\\nAntonio, for example, are just right. \\n\\nPutting it all together, as a material source of expected firm income, we invest our firm‚Äôs balance sheet in all \\n\\nStone Ridge strategies ‚Äì alternative lending, reinsurance, SFR, energy, art, market insurance, and bitcoin ‚Äì each \\n\\ndeveloped based on the foundational principles outlined above.\\n\\nOur empirical research shows that a hypothetical portfolio of all Stone Ridge strategies has produced meaningful\\n\\n[Source 3]: the seed investor and, whenever possible ‚Äì but not yet always ‚Äì the largest.  Consider our ‚Äúno-device‚Äù policy in \\n\\nthis context.  Would you ever pull out your phone and start scrolling in the middle of a meeting with your largest \\n\\ninvestor?  At Stone Ridge, we do not pull our phones out on each other.\\n\\nWith Stone Ridge increasingly Stone Ridge‚Äôs largest investor, the scale of our profits as principals, not agents, creates \\n\\na level of alignment with our outside investors we are unaware exists elsewhere.  Those investors occasionally \\n\\nremark that this is a wonderfully clarifying and palpable distinction, and that Stone Ridge feels different inside. \\n\\nEarlier this year, Stone Ridge Holdings Group (SRHG), parent company of Stone Ridge Asset Management, NYDIG, \\n\\nand Longtail Re, completed a significant primary equity financing led by an iconic American financial services firm\\n\\nNumber of relevant sources found: 3\\nRelevance scores: Source 1: 0.661, Source 2: 0.628, Source 3: 0.603\\n\\nQuestion: What is Stone Ridge's investment philosophy?\\n\\nPlease provide your answer based solely on the context above.\"}], 'system': 'You are a helpful investor letter assistant that answers questions about Stone Ridge\\'s investment philosophy, market insights, and strategic outlook based strictly on provided context.\\n\\nInstructions:\\n- Only answer questions using information from the provided context\\n- If the context doesn\\'t contain relevant information, respond with \"I don\\'t have information about that in the investor letter\"\\n- Be accurate and cite specific parts of the context when possible\\n- Keep responses detailed and efficient\\n- Only use the provided context. Do not use external knowledge.\\n- Include a reminder that this is for informational purposes only and not investment advice when appropriate\\n- Only provide answers when you are confident the context supports your response.'}\n",
      "{'role': 'user', 'content': \"Context Information:\\n[Source 1]: for our investors across all franchises, with no down months. While everyone at Stone Ridge can be proud of those \\n\\nnumbers, perspective is vital. \\n\\nFirst, nothing we‚Äôve achieved this year is because of this year. Success, and failure, are lagging indicators. Mountain \\n\\nclimbing disasters are always a series of small, seemingly inconsequential decisions that interact in unexpected \\n\\nways, compounding exponentially. Success works the same way. \\n\\nIt is fashionable these days to say investment outcomes follow a Power Law. That‚Äôs technically true and soulless. At \\n\\nStone Ridge, the compounding we‚Äôre after is in wisdom and trust in our relationships with each other. Life is much \\n\\nmore satisfying when we realize that relationships also follow a Power Law, and invest accordingly.\\n\\nSecond, we let the simplicity of our business model be enough. The humility of our questions and the plainness of our\\n\\n[Source 2]: particularly around known and unknown unknowns in stressed markets. We seek ‚Äújust right‚Äù for us. \\n\\nIn alternative lending, this means no AAA student loans (too cold) and no subprime or Emerging Markets (too \\n\\nhot). Prime, U.S.-only is just right. In SFR, this means no homes over $500k (too cold) or under $150k (too hot). It \\n\\nalso means no low-growth Detroit (too cold) and no boom/bust Seattle (too hot). $250-400k homes in steady San \\n\\nAntonio, for example, are just right. \\n\\nPutting it all together, as a material source of expected firm income, we invest our firm‚Äôs balance sheet in all \\n\\nStone Ridge strategies ‚Äì alternative lending, reinsurance, SFR, energy, art, market insurance, and bitcoin ‚Äì each \\n\\ndeveloped based on the foundational principles outlined above.\\n\\nOur empirical research shows that a hypothetical portfolio of all Stone Ridge strategies has produced meaningful\\n\\n[Source 3]: the seed investor and, whenever possible ‚Äì but not yet always ‚Äì the largest.  Consider our ‚Äúno-device‚Äù policy in \\n\\nthis context.  Would you ever pull out your phone and start scrolling in the middle of a meeting with your largest \\n\\ninvestor?  At Stone Ridge, we do not pull our phones out on each other.\\n\\nWith Stone Ridge increasingly Stone Ridge‚Äôs largest investor, the scale of our profits as principals, not agents, creates \\n\\na level of alignment with our outside investors we are unaware exists elsewhere.  Those investors occasionally \\n\\nremark that this is a wonderfully clarifying and palpable distinction, and that Stone Ridge feels different inside. \\n\\nEarlier this year, Stone Ridge Holdings Group (SRHG), parent company of Stone Ridge Asset Management, NYDIG, \\n\\nand Longtail Re, completed a significant primary equity financing led by an iconic American financial services firm\\n\\nNumber of relevant sources found: 3\\nRelevance scores: Source 1: 0.661, Source 2: 0.628, Source 3: 0.603\\n\\nQuestion: What is Stone Ridge's investment philosophy?\\n\\nPlease provide your answer based solely on the context above.\"}\n",
      "Response: Based on the investor letter, Stone Ridge's investment philosophy centers on several key principles:\n",
      "\n",
      "## Long-term Compounding and Relationships\n",
      "Stone Ridge emphasizes that \"Success, and failure, are lagging indicators\" - meaning current achievements stem from past decisions, not recent actions. The firm prioritizes **compounding wisdom and trust in relationships** over short-term results, recognizing that \"relationships also follow a Power Law\" and should be invested in accordingly.\n",
      "\n",
      "## Simplicity and Humility\n",
      "The firm maintains a simple business model with \"humility of our questions and the plainness of our\" approach (the quote continues beyond what's provided, but emphasizes letting simplicity be enough).\n",
      "\n",
      "## \"Just Right\" Risk Management\n",
      "Stone Ridge employs a Goldilocks approach to risk - avoiding extremes that are \"too cold\" (too conservative) or \"too hot\" (too risky):\n",
      "- **Alternative lending**: Prime, U.S.-only loans (avoiding AAA student loans and subprime/Emerging Markets)\n",
      "- **Single Family Rental (SFR)**: Homes priced $250-400k in steady markets like San Antonio (avoiding homes over $500k or under $150k, and avoiding boom/bust markets)\n",
      "\n",
      "## Principal-Led Investing\n",
      "Stone Ridge invests its own balance sheet in all its strategies (alternative lending, reinsurance, SFR, energy, art, market insurance, and bitcoin), serving as \"the seed investor and, whenever possible... the largest\" investor. This creates exceptional alignment with outside investors, as the firm profits as principals rather than agents.\n",
      "\n",
      "This philosophy aims to balance risk and return while maintaining strong alignment between the firm and its investors.\n",
      "\n",
      "*This is for informational purposes only and not investment advice.*\n",
      "\n",
      "Context Count: 3\n",
      "Similarity Scores: ['Source 1: 0.661', 'Source 2: 0.628', 'Source 3: 0.603']\n"
     ]
    }
   ],
   "source": [
    "rag_pipeline = RetrievalAugmentedQAPipeline(\n",
    "    vector_db_retriever=vector_db,\n",
    "    llm=chat_llm,\n",
    "    response_style=\"detailed\",\n",
    "    include_scores=True\n",
    ")\n",
    "\n",
    "result = rag_pipeline.run_pipeline(\n",
    "    \"What is Stone Ridge's investment philosophy?\",\n",
    "    k=3,\n",
    "    response_length=\"efficient\", \n",
    "    include_warnings=True,\n",
    "    confidence_required=True\n",
    ")\n",
    "\n",
    "print(result['prompts_used'][\"user\"])\n",
    "\n",
    "print(f\"Response: {result['response']}\")\n",
    "print(f\"\\nContext Count: {result['context_count']}\")\n",
    "print(f\"Similarity Scores: {result['similarity_scores']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZIJI19uR6yz"
   },
   "source": [
    "#### ‚ùì Question #4:\n",
    "\n",
    "What prompting strategies could you use to make the LLM have a more thoughtful, detailed response?\n",
    "\n",
    "What is that strategy called?\n",
    "\n",
    "> NOTE: You can look through our [OpenAI Responses API](https://colab.research.google.com/drive/14SCfRnp39N7aoOx8ZxadWb0hAqk4lQdL?usp=sharing) notebook for an answer to this question if you get stuck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### ‚úÖ Answer:\nTo make the LLM have a more thoughtful, detailed response, we can use Chain-of-Thought Prompting, as described in the assigned reading \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.\" This builds on few-shot prompting to provide examples that effectively have the LLM produce a result that reasons instead of just coming up with an answer. For example, if I say \"what's 2+4-1?\", the LLM might say \"7\", or it might not (probably will since this is a simplified example). But if I give an example of how to solve it in context that says \"This equation can be modified to ((2+4)-1) and executed in steps. (2+4) is (6). (6-1) is 5. The answer is 5.\" then the model will follow that line of reasoning on other problems and be more likely to get the answer right."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèóÔ∏è Activity #1:\n",
    "\n",
    "Enhance your Stone Ridge Investor Letter Assistant in some way! \n",
    "\n",
    "Suggestions are: \n",
    "\n",
    "- **Multi-Document Support**: Allow it to ingest multiple investor letters from different years for historical comparison\n",
    "- **New Distance Metric**: Implement a different similarity measure - does it improve retrieval for financial content?\n",
    "- **Metadata Support**: Add metadata like year, topic categories (market outlook, strategy, performance) to help filter results\n",
    "- **Different Embedding Model**: Try a different embedding model - does domain-specific tuning help for financial content?\n",
    "- **Multi-Source Ingestion**: Add the capability to ingest content from SEC filings, earnings calls, or other financial documents\n",
    "\n",
    "While these are suggestions, you should feel free to make whatever augmentations you desire! Think about what features would make this investor letter assistant most useful for understanding Stone Ridge's investment approach.\n",
    "\n",
    "When you're finished making the augments to your RAG application - vibe check it against the old one - see if you can \"feel the improvement\"!\n",
    "\n",
    "> NOTE: These additions might require you to work within the `aimakerspace` library - that's expected!\n",
    "\n",
    "> NOTE: If you're not sure where to start - ask Cursor (CMD/CTRL+L) to guide you through the changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### YOUR CODE HERE\n\n# =============================================================================\n# PREAMBLE\n# =============================================================================\n#\n# This solution implements several enhancements to the base RAG application:\n#\n# 1. PARAGRAPH-BASED CHUNKING (EXPERIMENTAL - DID NOT WORK WELL)\n#    Added a new \"paragraph\" split mode to CharacterTextSplitter. Since PDF\n#    extraction doesn't reliably preserve paragraph boundaries, we use \" \\n\"\n#    (space followed by newline) as an approximation. However, it was not\n#    possible to accurately split by paragraphs with the output of the PDF\n#    parser. Single newlines within chunks are replaced with literal \\n.\n#\n# 2. DATA PREPROCESSING/CLEANUP\n#    Pre-filtered documents to remove \"Risk Disclosures\" sections before chunking.\n#    Initial results were poor because legal boilerplate dominated the retrieved\n#    context. Other approaches tried (exclusion searches, downranking legalese\n#    keywords) were less effective than simply removing disclaimers upfront.\n#\n# 3. MULTI-DOCUMENT SUPPORT\n#    Added additional investor letters (2023, 2024, 2025) to the data directory\n#    and pointed PDFFileLoader to the parent directory. This enables historical\n#    comparison across multiple years without modifying the loader itself.\n#\n# 4. ALTERNATIVE DISTANCE METRIC\n#    Added toggle between cosine similarity and Pearson correlation via the\n#    DISTANCE_METRIC constant. Both performed similarly on this dataset - cosine\n#    is generally optimal for normalized embeddings, but Pearson can capture\n#    relationships cosine might miss.\n#\n# 5. SOURCE METADATA\n#    Added metadata support to track which document each chunk originated from.\n#    The VectorDatabase now stores (vector, metadata) tuples, and search results\n#    include source information. Context sent to the LLM now shows [Source: filename]\n#    for each chunk, enabling the model to cite specific documents in its answers.\n#\n# 6. PER-DOCUMENT CONTEXT RETRIEVAL\n#    Added CONTEXT_PER_DOCUMENT flag to retrieve N chunks from each source document\n#    separately, rather than the top N globally. This is useful for comparison\n#    questions across documents (e.g., \"How has Stone Ridge's philosophy changed\n#    over time?\"). When enabled, NUM_CONTEXT_CHUNKS specifies the number of chunks\n#    to retrieve per document. Results are sorted using a two-step sort: first by\n#    source name to group chunks from the same document together, then by score\n#    descending within each group.\n#\n\n\n# =============================================================================\n# CONSTANTS\n# =============================================================================\n\n# Maximum number of characters per chunk\nCHUNK_SIZE = 1000\n\n# Number of overlapping characters between consecutive chunks (helps preserve context at boundaries)\nCHUNK_OVERLAP = 200\n\n# Chunking strategy: \"character\" splits at fixed intervals, \"paragraph\" splits on newlines\nSPLIT_MODE = \"character\"\n\n# Number of most similar chunks to retrieve (per document if CONTEXT_PER_DOCUMENT is True)\nNUM_CONTEXT_CHUNKS = 3\n\n# When True, retrieve NUM_CONTEXT_CHUNKS from each source document separately\nCONTEXT_PER_DOCUMENT = True\n\n# Response style passed to the LLM (e.g., \"detailed\", \"concise\", \"technical\")\nRESPONSE_STYLE = \"detailed\"\n\n# Response length passed to the LLM (e.g., \"brief\", \"efficient\", \"comprehensive\")\nRESPONSE_LENGTH = \"efficient\"\n\n# Default question to ask the RAG application\n#QUERY = \"What is Stone Ridge's investment philosophy?\"\nQUERY = \"Has Stone Ridge's investment philosophy evolved over the years?\"\n\n\n# =============================================================================\n# IMPORTS\n# =============================================================================\n\nimport os\nimport certifi\nimport asyncio\nimport nest_asyncio\n\nfrom aimakerspace.text_utils import PDFFileLoader, CharacterTextSplitter\nfrom aimakerspace.vectordatabase import VectorDatabase, cosine_similarity, pearson_correlation\nfrom aimakerspace.ai_utils.prompts import UserRolePrompt, SystemRolePrompt\nfrom aimakerspace.ai_utils.chatmodel import ChatAnthropic\n\nnest_asyncio.apply()\n\n# Distance metric for vector similarity: cosine_similarity or pearson_correlation\nDISTANCE_METRIC = pearson_correlation\n\n\n# =============================================================================\n# PROMPT TEMPLATES\n# =============================================================================\n\nRAG_SYSTEM_TEMPLATE = \"\"\"You are a helpful investor letter assistant that answers questions about Stone Ridge's investment philosophy, market insights, and strategic outlook based strictly on provided context.\n\nInstructions:\n- Only answer questions using information from the provided context\n- If the context doesn't contain relevant information, respond with \"I don't have information about that in the investor letter\"\n- Be accurate and cite specific parts of the context when possible\n- Keep responses {response_style} and {response_length}\n- Only use the provided context. Do not use external knowledge.\n- Include a reminder that this is for informational purposes only and not investment advice when appropriate\n- Only provide answers when you are confident the context supports your response.\"\"\"\n\nRAG_USER_TEMPLATE = \"\"\"Context Information:\n{context}\n\nNumber of relevant sources found: {context_count}\n{similarity_scores}\n\nQuestion: {user_query}\n\nPlease provide your answer based solely on the context above.\"\"\"\n\n\n# =============================================================================\n# CLASSES\n# =============================================================================\n\nclass RetrievalAugmentedQAPipeline:\n    def __init__(self, llm: ChatAnthropic, vector_db_retriever: VectorDatabase, \n                 response_style: str = \"detailed\", include_scores: bool = False) -> None:\n        self.llm = llm\n        self.vector_db_retriever = vector_db_retriever\n        self.response_style = response_style\n        self.include_scores = include_scores\n        self.rag_system_prompt = SystemRolePrompt(RAG_SYSTEM_TEMPLATE)\n        self.rag_user_prompt = UserRolePrompt(RAG_USER_TEMPLATE)\n\n    def run_pipeline(self, user_query: str, k: int = 3, distance_metric=None, **system_kwargs) -> dict:\n        if distance_metric is None:\n            distance_metric = DISTANCE_METRIC\n        \n        if CONTEXT_PER_DOCUMENT:\n            # Get unique sources and retrieve k chunks from each (sorted for chronological order)\n            sources = sorted(self.vector_db_retriever.get_unique_metadata_values(\"source\"))\n            context_list = []\n            for source in sources:\n                results = self.vector_db_retriever.search_by_text(\n                    user_query, k=k, distance_metric=distance_metric,\n                    metadata_filter={\"source\": source}\n                )\n                context_list.extend(results)\n            # Sort by source to group chunks together, then by score within each group\n            context_list = sorted(context_list, key=lambda x: (x[2].get(\"source\", \"\"), -x[1]))\n        else:\n            context_list = self.vector_db_retriever.search_by_text(\n                user_query, k=k, distance_metric=distance_metric\n            )\n            context_list = sorted(context_list, key=lambda x: x[1], reverse=True)\n\n        context_prompt = \"\"\n        similarity_scores = []\n        \n        for i, (context, score, metadata) in enumerate(context_list, 1):\n            source = metadata.get(\"source\", \"Unknown\")\n            context_prompt += f\"[Source: {source}]\\n{context}\\n\\n---\\n\\n\"\n            similarity_scores.append(f\"Source {i}: {score:.3f}\")\n        \n        # Log the context being sent to the LLM\n        print(\"=\" * 80)\n        print(\"CONTEXT SENT TO LLM:\")\n        print(\"=\" * 80)\n        print(context_prompt)\n        \n        system_params = {\n            \"response_style\": self.response_style,\n            \"response_length\": system_kwargs.get(\"response_length\", \"detailed\")\n        }\n        \n        formatted_system_prompt = self.rag_system_prompt.create_message(**system_params)\n        \n        user_params = {\n            \"user_query\": user_query,\n            \"context\": context_prompt.strip(),\n            \"context_count\": len(context_list),\n            \"similarity_scores\": f\"Relevance scores: {', '.join(similarity_scores)}\" if self.include_scores else \"\"\n        }\n        \n        formatted_user_prompt = self.rag_user_prompt.create_message(**user_params)\n\n        return {\n            \"response\": self.llm.run([formatted_system_prompt, formatted_user_prompt]), \n            \"context\": context_list,\n            \"context_count\": len(context_list),\n            \"similarity_scores\": similarity_scores if self.include_scores else None,\n        }\n\n\n# =============================================================================\n# FUNCTIONS\n# =============================================================================\n\ndef zscaler_ssl_setup():\n    \"\"\"Configure SSL certificates to work with Zscaler corporate network.\"\"\"\n    zscaler_cert = \"/Users/ari.packer/repos/sidekick/zscaler.pem\"\n    combined_cert = \"/tmp/combined_certs.pem\"\n\n    with open(combined_cert, \"w\") as outfile:\n        with open(certifi.where(), \"r\") as certifi_file:\n            outfile.write(certifi_file.read())\n        with open(zscaler_cert, \"r\") as zscaler_file:\n            outfile.write(zscaler_file.read())\n\n    os.environ['REQUESTS_CA_BUNDLE'] = combined_cert\n    os.environ['SSL_CERT_FILE'] = combined_cert\n    os.environ['CURL_CA_BUNDLE'] = combined_cert\n\n\ndef run_rag_application():\n    \"\"\"Build and run the RAG application for Stone Ridge investor letters.\"\"\"\n    documents = load_documents(\"data\")\n    print(f\"Loaded {len(documents)} documents\")\n\n    split_documents, metadata_list = chunk_documents(documents)\n    print(f\"Split into {len(split_documents)} chunks\")\n\n    vector_db = init_vector_db(split_documents, metadata_list)\n    print(f\"Vector database built with {len(vector_db.vectors)} vectors\")\n\n    chat_llm = ChatAnthropic()\n\n    rag_pipeline = RetrievalAugmentedQAPipeline(\n        vector_db_retriever=vector_db,\n        llm=chat_llm,\n        response_style=RESPONSE_STYLE,\n        include_scores=True\n    )\n\n    result = rag_pipeline.run_pipeline(\n        QUERY,\n        k=NUM_CONTEXT_CHUNKS,\n        response_length=RESPONSE_LENGTH\n    )\n\n    print(\"=\" * 80)\n    print(\"RESPONSE:\")\n    print(\"=\" * 80)\n    print(result['response'])\n    print(\"\\n\")\n    print(f\"Context Count: {result['context_count']}\")\n    print(f\"Similarity Scores: {result['similarity_scores']}\")\n\n\ndef load_documents(path: str) -> list:\n    \"\"\"Load PDF documents and filter out legal disclaimers.\"\"\"\n    pdf_loader = PDFFileLoader(path)\n    documents = pdf_loader.load_documents()\n    # Filter out Risk Disclosures while preserving metadata\n    documents = [(text.split('Risk Disclosures')[0], source) for text, source in documents]\n    return documents\n\n\ndef chunk_documents(documents: list) -> tuple:\n    \"\"\"Split documents into chunks using paragraph-based splitting.\"\"\"\n    text_splitter = CharacterTextSplitter(CHUNK_SIZE, CHUNK_OVERLAP, SPLIT_MODE)\n    split_documents, metadata_list = text_splitter.split_texts_with_metadata(documents)\n    return split_documents, metadata_list\n\n\ndef init_vector_db(split_documents: list, metadata_list: list) -> VectorDatabase:\n    \"\"\"Initialize vector database with document embeddings and metadata.\"\"\"\n    vector_db = VectorDatabase()\n    vector_db = asyncio.run(vector_db.abuild_from_list(split_documents, metadata_list))\n    return vector_db\n\n\n# =============================================================================\n# MAIN\n# =============================================================================\n\nzscaler_ssl_setup()\nrun_rag_application()"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "02-embeddings-and-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}